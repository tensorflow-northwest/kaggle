{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating predictions\n",
    "\n",
    "This is the general structure for submitting predictions for the iMaterialist competition. It will be included in the master colab notebook.\n",
    "\n",
    "First, a quick function to load the test dataset. In the future this will be one file that contains all 12,800 images. I have tried to make this already but the file size of the final .h5 file is huge for some reason. Merits more investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_final_testset(filename):\n",
    "    with h5py.File(filename, 'r') as hf:\n",
    "        test_images = hf['test_dataset'][:]\n",
    "        test_images = test_images.astype('float32', copy=False)\n",
    "        test_images = preprocess_input(test_images) # preprocess_input is a Keras function\n",
    "\n",
    "    return test_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: We still have to generate a prediction for the 106 missing test images. When I was constructing `test_images_106_missing.h5` if an image was not found (e.g., `156.jpg`) I generated a random numpy array with `np.random.randint(255, shape=(224, 224, 3))` in its place. Thus, `test_images_106_missing.h5` had 12800 entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = 'test_images_106_missing.h5'\n",
    "x_test_final = load_final_testset(test_file)\n",
    "predictions = model.predict(x_test_final, verbose=1, batch_size=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since our model contains an auxiliary branch it returns predictions for both branches. I'll just grab predictions for the main branch. Also we need to submit one prediction for each test image, so I'll use `argmax()` to get the most likely category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predictions[0].argmax(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to save the predictions as a .csv file. Per the [sample submission](https://www.kaggle.com/c/imaterialist-challenge-furniture-2018/data) on Kaggle for this competition, the .csv file needs to columns, one for `id`, and one for `predicted`. I'm saving these predicitions in a new directory and attaching the timestamp (in UTC) to the name of the file. That way we don't overwrite any of our submission files.\n",
    "\n",
    "In the future we might want to also include the validation set accuracy in the filename so we know which .csv file we want to submit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # dataframes\n",
    "\n",
    "df = pd.DataFrame({'id' : range(1, len(predictions) + 1), \n",
    "                   'predicted' : predictions})\n"
    "df.loc[df.predicted == 0, 'predicted'] = 128\n"
    "if not os.path.isdir('predictions'):\n",
    "  os.mkdir('predictions')\n",
    "filename = 'predictions/predictions_' + time.strftime('%Y%m%d_%H%M%S_%Z.csv')\n",
    "df.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
